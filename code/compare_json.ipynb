{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Accuracy Report\n",
                "This notebook provides a visual analysis of the extraction accuracy compared to the ground truth across multiple documents."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting matplotlib\n",
                        "  Downloading matplotlib-3.10.8-cp313-cp313-macosx_11_0_arm64.whl.metadata (52 kB)\n",
                        "Collecting contourpy>=1.0.1 (from matplotlib)\n",
                        "  Downloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
                        "Collecting cycler>=0.10 (from matplotlib)\n",
                        "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
                        "Collecting fonttools>=4.22.0 (from matplotlib)\n",
                        "  Downloading fonttools-4.61.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (114 kB)\n",
                        "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
                        "  Downloading kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
                        "Requirement already satisfied: numpy>=1.23 in /Users/pat/Desktop/custom_FM/.venv/lib/python3.13/site-packages (from matplotlib) (2.4.0)\n",
                        "Requirement already satisfied: packaging>=20.0 in /Users/pat/Desktop/custom_FM/.venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
                        "Requirement already satisfied: pillow>=8 in /Users/pat/Desktop/custom_FM/.venv/lib/python3.13/site-packages (from matplotlib) (12.0.0)\n",
                        "Collecting pyparsing>=3 (from matplotlib)\n",
                        "  Downloading pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
                        "Requirement already satisfied: python-dateutil>=2.7 in /Users/pat/Desktop/custom_FM/.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
                        "Requirement already satisfied: six>=1.5 in /Users/pat/Desktop/custom_FM/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
                        "Downloading matplotlib-3.10.8-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl (274 kB)\n",
                        "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
                        "Downloading fonttools-4.61.1-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl (64 kB)\n",
                        "Downloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
                        "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [matplotlib]6\u001b[0m [matplotlib]\n",
                        "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pyparsing-3.3.1\n"
                    ]
                }
            ],
            "source": [
                "!pip install matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Warning: No Ground Truth found for INT-001_V2\n",
                        "Warning: No Ground Truth found for LZD-001_V2\n",
                        "Warning: No Ground Truth found for MSV-001_V2\n",
                        "Warning: No Ground Truth found for PPP-001_V2\n",
                        "Warning: No Ground Truth found for PPP-002_V2\n",
                        "Warning: No Ground Truth found for PTV-001_V2\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "import difflib\n",
                "import os\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "# Global list to store summary results\n",
                "batch_summary = []\n",
                "\n",
                "def calculate_accuracy(str1, str2):\n",
                "    str1 = str(str1).strip()\n",
                "    str2 = str(str2).strip()\n",
                "    if not str1 and not str2:\n",
                "        return 100.0\n",
                "    matcher = difflib.SequenceMatcher(None, str1, str2)\n",
                "    return round(matcher.ratio() * 100, 2)\n",
                "\n",
                "def get_comparison_data(gt_path, res_path):\n",
                "    with open(gt_path, 'r', encoding='utf-8') as f:\n",
                "        gt = json.load(f)\n",
                "    with open(res_path, 'r', encoding='utf-8') as f:\n",
                "        res = json.load(f)\n",
                "\n",
                "    data = []\n",
                "    \n",
                "    # Top-level fields\n",
                "    for key in gt.keys():\n",
                "        if key == 'line_items': continue\n",
                "        val_gt = gt.get(key)\n",
                "        val_res = res.get(key, \"\")\n",
                "        \n",
                "        if not val_gt or str(val_gt).strip() == \"\":\n",
                "            continue\n",
                "            \n",
                "        acc = calculate_accuracy(val_gt, val_res)\n",
                "        data.append({\"Field\": key, \"Accuracy\": acc, \"Ground Truth\": val_gt, \"Result\": val_res, \"Category\": \"Header\"})\n",
                "\n",
                "    # Line items\n",
                "    gt_items = gt.get('line_items', [])\n",
                "    res_items = res.get('line_items', [])\n",
                "    for i, item_gt in enumerate(gt_items):\n",
                "        item_res = res_items[i] if i < len(res_items) else {}\n",
                "        for k, v_gt in item_gt.items():\n",
                "            if not v_gt or str(v_gt).strip() == \"\": continue\n",
                "            v_res = item_res.get(k, \"\")\n",
                "            acc = calculate_accuracy(v_gt, v_res)\n",
                "            data.append({\"Field\": f\"Item_{i+1}_{k}\", \"Accuracy\": acc, \"Ground Truth\": v_gt, \"Result\": v_res, \"Category\": \"Line Item\"})\n",
                "            \n",
                "    return pd.DataFrame(data)\n",
                "\n",
                "def generate_visual_report(df, filename):\n",
                "    if df.empty:\n",
                "        print(f\"No data to report for {filename}.\")\n",
                "        return\n",
                "\n",
                "    overall_acc = df['Accuracy'].mean()\n",
                "    \n",
                "    # Store in summary\n",
                "    batch_summary.append({\"File\": filename, \"Score\": overall_acc, \"Fields\": len(df)})\n",
                "\n",
                "    display(HTML(f\"<h1 style='border-bottom: 2px solid #334155; padding-bottom: 10px; margin-top: 50px;'>Report: {filename}</h1>\"))\n",
                "\n",
                "    # 1. Summary Card\n",
                "    summary_html = f\"\"\"\n",
                "    <div style=\"padding: 20px; border-radius: 10px; background-color: #f8fafc; border-left: 8px solid #4CAF50; margin-bottom: 24px; box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1);\">\n",
                "        <h2 style=\"margin: 0; color: #1e293b; font-family: sans-serif;\">Extraction Accuracy Summary</h2>\n",
                "        <div style=\"display: flex; align-items: center; margin-top: 12px;\">\n",
                "            <span style=\"font-size: 16px; color: #64748b; margin-right: 8px;\">Overall Score:</span>\n",
                "            <span style=\"color: #16a34a; font-size: 28px; font-weight: bold;\">{overall_acc:.2f}%</span>\n",
                "        </div>\n",
                "        <p style=\"margin: 8px 0 0 0; color: #64748b; font-size: 14px;\">Total Fields Evaluated: <b style=\"color: #334155;\">{len(df)}</b></p>\n",
                "    </div>\n",
                "    \"\"\"\n",
                "    display(HTML(summary_html))\n",
                "\n",
                "    # 2. Bar Chart\n",
                "    plt.figure(figsize=(10, len(df) * 0.45))\n",
                "    colors = ['#22c55e' if x > 95 else '#84cc16' if x > 90 else '#eab308' if x > 70 else '#ef4444' for x in df['Accuracy']]\n",
                "    plt.barh(df['Field'], df['Accuracy'], color=colors, height=0.7)\n",
                "    plt.xlabel('Accuracy %')\n",
                "    plt.title(f'Field Accuracy - {filename}')\n",
                "    plt.xlim(0, 110)\n",
                "    plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
                "    plt.gca().invert_yaxis()\n",
                "    plt.gca().spines['top'].set_visible(False)\n",
                "    plt.gca().spines['right'].set_visible(False)\n",
                "    for i, v in enumerate(df['Accuracy']):\n",
                "        plt.text(v + 1, i, f\"{v}%\", color='#334155', va='center')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "    # 3. Styled Table\n",
                "    def style_accuracy_col(val):\n",
                "        if val > 95: bg, text = '#f0fdf4', '#166534'\n",
                "        elif val > 90: bg, text = '#ecfdf5', '#065f46'\n",
                "        elif val > 70: bg, text = '#fffbeb', '#92400e'\n",
                "        else: bg, text = '#fef2f2', '#991b1b'\n",
                "        return f'background-color: {bg}; color: {text}; font-weight: bold;'\n",
                "\n",
                "    styled_df = df.style.map(style_accuracy_col, subset=['Accuracy'])\\\n",
                "                       .format({\"Accuracy\": \"{:.2f}%\"})\\\n",
                "                       .set_properties(**{'text-align': 'left', 'padding': '12px 15px', 'border-bottom': '1px solid #e2e8f0', 'font-family': 'sans-serif'})\\\n",
                "                       .set_table_styles([\n",
                "                           {'selector': 'th', 'props': [('background-color', '#f8fafc'), ('color', '#475569'), ('font-weight', 'bold'), ('text-transform', 'uppercase'), ('font-size', '12px')]},\n",
                "                           {'selector': 'tr:hover', 'props': [('background-color', '#f1f5f9')]}\n",
                "                       ])\n",
                "    \n",
                "    display(styled_df)\n",
                "\n",
                "# Execution Loop\n",
                "result_dir = '/Users/pat/Desktop/custom_FM/working/comparison/result/updated_prompt_result/'\n",
                "gt_dir = '/Users/pat/Desktop/custom_FM/working/comparison/ground_truth/converted/'\n",
                "\n",
                "batch_summary = [] # Reset summary\n",
                "\n",
                "for res_file in sorted(os.listdir(result_dir)):\n",
                "    if res_file.startswith(\"output_\") and res_file.endswith(\".json\"):\n",
                "        # Identify the ID (e.g., MSV-001)\n",
                "        file_id = res_file.replace(\"output_\", \"\").replace(\".json\", \"\")\n",
                "        gt_file = f\"{file_id}.json\"\n",
                "        \n",
                "        res_path = os.path.join(result_dir, res_file)\n",
                "        gt_path = os.path.join(gt_dir, gt_file)\n",
                "        \n",
                "        if os.path.exists(gt_path):\n",
                "            df_results = get_comparison_data(gt_path, res_path)\n",
                "            generate_visual_report(df_results, file_id)\n",
                "        else:\n",
                "            print(f\"Warning: No Ground Truth found for {file_id}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<h1 style='color: #1e293b; margin-top: 40px;'>Batch Processing Final Results</h1>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "No results to summarize.\n"
                    ]
                }
            ],
            "source": [
                "# FINAL BATCH SUMMARY\n",
                "display(HTML(\"<h1 style='color: #1e293b; margin-top: 40px;'>Batch Processing Final Results</h1>\"))\n",
                "\n",
                "summary_df = pd.DataFrame(batch_summary)\n",
                "\n",
                "if not summary_df.empty:\n",
                "    # Aggregate Score\n",
                "    avg_batch_score = summary_df['Score'].mean()\n",
                "    \n",
                "    display(HTML(f\"\"\"\n",
                "    <div style='background: linear-gradient(135deg, #1e293b 0%, #334155 100%); color: white; padding: 30px; border-radius: 15px; margin-bottom: 30px;'>\n",
                "        <h2 style='margin: 0; opacity: 0.8;'>Aggregate Accuracy</h2>\n",
                "        <div style='font-size: 48px; font-weight: bold;'>{avg_batch_score:.2f}%</div>\n",
                "        <div style='margin-top: 10px; font-size: 16px;'>Across {len(summary_df)} documents</div>\n",
                "    </div>\n",
                "    \"\"\"))\n",
                "\n",
                "    # Plotting Leaderboard\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    plt.bar(summary_df['File'], summary_df['Score'], color='#6366f1')\n",
                "    plt.axhline(y=avg_batch_score, color='#ef4444', linestyle='--', label=f'Average ({avg_batch_score:.1f}%)')\n",
                "    plt.title(\"Comparison Score per Document\", fontsize=15, fontweight='bold')\n",
                "    plt.ylabel(\"Accuracy %\")\n",
                "    plt.ylim(0, 110)\n",
                "    plt.legend()\n",
                "    plt.grid(axis='y', alpha=0.2)\n",
                "    plt.show()\n",
                "\n",
                "    # Detailed Final Styler\n",
                "    def final_score_style(val):\n",
                "        color = '#16a34a' if val > 90 else '#eab308' if val > 75 else '#ef4444'\n",
                "        return f'color: {color}; font-weight: bold;'\n",
                "\n",
                "    # Display final results table\n",
                "    display(HTML(\"<h3>Document Leaderboard</h3>\"))\n",
                "    display(summary_df.style.map(final_score_style, subset=['Score'])\\\n",
                "                      .format({\"Score\": \"{:.2f}%\"})\\\n",
                "                      .set_properties(**{'text-align': 'center', 'padding': '15px'})\\\n",
                "                      .hide(axis='index'))\n",
                "else:\n",
                "    print(\"No results to summarize.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
